{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "LSTM_Model.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt160Wgi7NRa",
        "colab_type": "text"
      },
      "source": [
        "## UserIdentification Using LSTM.\n",
        "\n",
        "\n",
        "*   preprocessing of data is done ofline\n",
        "*   Run it on TPU to get best training time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2uM2a9aNAkxY"
      },
      "source": [
        "Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RkyBwrFTAkxa",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import gc\n",
        "import nltk\n",
        "import glob\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import LSTM,Dense,Embedding,SpatialDropout1D,Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0KKXefUMp46"
      },
      "source": [
        "# TPU Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sk6aRcGeMtuH",
        "colab": {}
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aOJI2wBRNBa4"
      },
      "source": [
        "TPU statergy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QVAechJeM_TI",
        "colab": {}
      },
      "source": [
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4os4SXxZpHCT"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-CrNZNMApOax"
      },
      "source": [
        "Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ouKp_k4BN1B",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive') #give authentication to access drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T4BdDKwZAk0B",
        "colab": {}
      },
      "source": [
        "input_data=pd.read_csv('/content/drive/My Drive/LSTM/inputdata.csv') #read csv file\n",
        "print(input_data.head())\n",
        "print(input_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n0qCJY4hAk0T",
        "colab": {}
      },
      "source": [
        "input_data['tweets'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-l9rzZ7Akz8"
      },
      "source": [
        "# Create a Vocabulary and Vectors of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6oZXcUMYAk1W",
        "colab": {}
      },
      "source": [
        "\n",
        "input_data=shuffle(input_data)\n",
        "input_data['tweets'] = input_data['tweets'].apply(lambda x: x.lower()) #convert all uppercase to lower\n",
        "input_data['tweets'] = input_data['tweets'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) #remove all special characters(if any)\n",
        "train,test=train_test_split(input_data,random_state=42,test_size=0.002)\n",
        "print(train.shape,test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6IbL4aMdAk1v",
        "colab": {}
      },
      "source": [
        "maxlen = -1\n",
        "for i in train['tweets']:       #find length of longest tweet\n",
        "  try:\n",
        "    if len(i.split()) > maxlen:\n",
        "      maxlen = len(i.split())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "print(maxlen)\n",
        "#Tokenize data and convert it into vectors\n",
        "tokenizer = Tokenizer(split=' ')\n",
        "tokenizer.fit_on_texts(input_data['tweets'].values)\n",
        "count_thres = 5\n",
        "low_count_words = [w for w,c in tokenizer.word_counts.items() if c < count_thres]\n",
        "#print(tokenizer.texts_to_sequences(comments))\n",
        "for w in low_count_words:\n",
        "    del tokenizer.word_index[w]\n",
        "    del tokenizer.word_docs[w]\n",
        "    del tokenizer.word_counts[w]\n",
        "train_x = tokenizer.texts_to_sequences(train['tweets'].values)\n",
        "train_x = pad_sequences(train_x,maxlen=maxlen)\n",
        "test_x=tokenizer.texts_to_sequences(test['tweets'].values)\n",
        "test_x=pad_sequences(test_x,maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rk1q61BWAk2A",
        "colab": {}
      },
      "source": [
        "print('Number of Tokens:', len(tokenizer.word_index))\n",
        "print(\"Max Token Index:\", train_x.max(), \"\\n\")\n",
        "train_x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LmqLQrAwAk2N",
        "colab": {}
      },
      "source": [
        "encoder = LabelEncoder()  #encode usernames\n",
        "\n",
        "encoder.fit(input_data['Username'].values)\n",
        "#test_y = encoder.fit(test['Username'].values)\n",
        "train_y= encoder.transform(train['Username'].values)\n",
        "test_y = encoder.transform(test['Username'].values)\n",
        "output_num=len(encoder.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p4W9B6ZPAk2n",
        "colab": {}
      },
      "source": [
        "len(train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oafcvJS4y9uu",
        "colab": {}
      },
      "source": [
        "len(train_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTd-0WgnKLiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BC2GiNaHyiBD"
      },
      "source": [
        "get Glove weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5YAgQNvDyVVp",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  !mkdir glove\n",
        "  !unzip '/content/drive/My Drive/LSTM/glove.twitter.27B.zip' -d glove/\n",
        "  #!unzip '/content/drive/My Drive/LSTM/glove.840B.300d.zip' -d glove/\n",
        "except:\n",
        "  pass\n",
        "  \n",
        "glove_embedding_length = 100   \n",
        "\n",
        "# Create GloVe Embeddings\n",
        "embedding_vector = {}\n",
        "f = open('glove/glove.twitter.27B.'+str(glove_embedding_length)+'d.txt',encoding='utf8')\n",
        "#f = open('glove/glove.840B.300d.txt',encoding='utf8')\n",
        "for line in f:\n",
        "    value = line.split(' ')\n",
        "    word = value[0]\n",
        "    coef = np.array(value[1:],dtype = 'float32')\n",
        "    embedding_vector[word] = coef\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, glove_embedding_length))\n",
        "for word,i in tokenizer.word_index.items():\n",
        "        embedding_value = embedding_vector.get(word)\n",
        "        if embedding_value is not None:\n",
        "            embedding_matrix[i] = embedding_value\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4T657GdKLiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(encoder.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAMqeg1oamPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  del(input_data,test,train) #delete unwanted variables \n",
        "except:\n",
        "  pass\n",
        "try:\n",
        "  del(f) #delete unwanted variables \n",
        "except:\n",
        "  pass\n",
        "gc.collect\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zSHALDmjbbD6"
      },
      "source": [
        "# LSTM on TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4wKYOME60xD",
        "colab_type": "text"
      },
      "source": [
        "# Model 1\n",
        "\n",
        "\n",
        "1.   Accuracy≈40% (30 epochs)\n",
        "2.   Time per epoch ≈ 10mins (5hours on GPU)\n",
        "3. High  RAM usage, might crash after 15-20 apochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CSptdUG2NdYa",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  del(model,history,hist_df,result) #delete anny previous model and history if any\n",
        "  gc.collect()\n",
        "except:\n",
        "  pass\n",
        "with strategy.scope():\n",
        "    model=tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(maxlen,)))\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, glove_embedding_length,weights=[embedding_matrix]))\n",
        "    model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
        "    model.add(tf.keras.layers.LSTM(768, dropout=0.1,activation='tanh',return_sequences=True))\n",
        "    model.add(tf.keras.layers.LSTM(1024, activation='tanh'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(768, activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(216))\n",
        "    model.add(tf.keras.layers.Dense(1024,activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(output_num,activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=tf.optimizers.Adam(0.00001),loss='sparse-categorical-crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history=model.fit(np.array(train_x),np.array(train_y), epochs=15, batch_size=64*statergy.num_replicas_in_sync)\n",
        "result=model.evaluate(np.array(test_x),np.array(test_y),batch_size=4*statergy.num_replicas_in_sync)\n",
        "\n",
        "try:\n",
        "  hist_df = pd.DataFrame(history.history)\n",
        "  hist_dt.to_csv('/content/drive/My Drive/LSTM/history_model1.csv')\n",
        "except:\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPKkcxwM1E41",
        "colab_type": "text"
      },
      "source": [
        "# Model 2\n",
        "\n",
        "\n",
        "\n",
        "1.   Accuracy ≈ \n",
        "             test 1:31-32%( 30+ expected (10 epochs), lr=0.0001,dense1=216,3LSTM layers, lstm1=256,lstm2==512,lstm3=768)\n",
        "             test 2: 44-45%(25+ expected(10epochs), lr=0.0003, dense1=256, , 4 lstmlayers, lstm1=256,lstm2=512,lstm3=512,lstm4=256)\n",
        "             Test 3: (32+ expected(15epochs),added lstm5 layer=256.)\n",
        "\n",
        "\n",
        "2.   Time per epoch ≈ \n",
        "                      1: 13mins\n",
        "                      2: 9mins\n",
        "                      3: 13mins\n",
        "3. Might Crash on Google colab due to high ram usage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v8K7zH5VGtU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkzMgrs0xAdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgBZDoQye3X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#baseline\n",
        "try:\n",
        "  del(model,history)\n",
        "except:\n",
        "  pass\n",
        "with strategy.scope():\n",
        "    model= tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(maxlen,)))\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, glove_embedding_length,weights=[embedding_matrix],input_length=maxlen,trainable=False))\n",
        "    model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, dropout=0.1, recurrent_dropout=0.1,activation='tanh',return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768,  activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    #model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='tanh',return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))       \n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(768,activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(int(output_num),activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer=tf.optimizers.Adam(0.0003), loss='sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
        "model.summary()\n",
        "history=model.fit(np.array(train_x),np.array(train_y), epochs=15, batch_size=256*strategy.num_replicas_in_sync,validation_data=(np.array(test_x),np.array(test_y)))\n",
        "\n",
        "#result=model.evaluate(np.array(test_x),np.array(test_y),batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV1LwQsRyh93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  del(model,history)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "lr_schedule=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.0005, decay_steps=, decay_rate=, staircase=False, name=None\n",
        ")\n",
        "\n",
        "with strategy.scope():\n",
        "    model= tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(maxlen,)))\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, glove_embedding_length,weights=[embedding_matrix],input_length=maxlen,trainable=False))\n",
        "    model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, dropout=0.1, recurrent_dropout=0.1,activation='tanh',return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024,  activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    #model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='tanh',return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    #model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "    #model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(1024,activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(int(output_num),activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer=tf.optimizers.Adam(lr_rate=), loss='sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
        "model.summary()\n",
        "history=model.fit(np.array(train_x),np.array(train_y), epochs=15, batch_size=400*strategy.num_replicas_in_sync,validation_data=(np.array(test_x),np.array(test_y)))\n",
        "\n",
        "#result=model.evaluate(np.array(test_x),np.array(test_y),batch_size=64)\n",
        "'''\n",
        "try:\n",
        "  hist_df = pd.DataFrame(history.history)\n",
        "  hist_dt.to_csv('/content/drive/My Drive/LSTM/history_model1.csv')\n",
        "except:\n",
        "  pass\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RzFYxxLBpaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOggCD0LTYnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  del(model,history,result)\n",
        "except:\n",
        "  pass\n",
        "with strategy.scope():\n",
        "    model= tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=(maxlen,)))\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, glove_embedding_length,weights=[embedding_matrix],input_length=maxlen,trainable=False))\n",
        "    model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, dropout=0.1, recurrent_dropout=0.1,activation='tanh',return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(768,  activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, activation='tanh', return_sequences=True),merge_mode='concat'))\n",
        "    #model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='tanh',return_sequences=True),merge_mode='concat'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    #model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(768,activation='relu'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(int(output_num),activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer=tf.optimizers.Adam(0.00045), loss='sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
        "model.summary()\n",
        "history=model.fit(np.array(train_x),np.array(train_y), epochs=15, batch_size=384*strategy.num_replicas_in_sync,validation_data=(np.array(test_x),np.array(test_y)))\n",
        "\n",
        "#result=model.evaluate(np.array(test_x),np.array(test_y),batch_size=64)\n",
        "try:\n",
        "  hist_df = pd.DataFrame(history.history)\n",
        "  hist_dt.to_csv('/content/drive/My Drive/LSTM/history_model2.csv')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHfM-hrcS2bH",
        "colab_type": "text"
      },
      "source": [
        "#Possible Reasons of Error\n",
        "\n",
        "\n",
        "*   Too large output\n",
        "*   Ram utilized by unnessary variables is high\n",
        "* Model not optimized.\n",
        "\n"
      ]
    }
  ]
}